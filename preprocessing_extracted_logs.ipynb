{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: extracted/openstack_train_extracted.csv\n",
      "Preprocessed file saved to: preprocessed/preprocessed_openstack_train.csv\n",
      "Processing file: extracted/openstack_test_extracted.csv\n",
      "Preprocessed file saved to: preprocessed/preprocessed_openstack_test.csv\n",
      "Processing file: extracted/openstack_predict_extracted.csv\n",
      "Preprocessed file saved to: preprocessed/preprocessed_openstack_predict.csv\n"
     ]
    }
   ],
   "source": [
    "def preprocess_logs(input_dir='extracted', output_dir='preprocessed'):\n",
    "  \"\"\"\n",
    "  Preprocesses multiple processed log datasets in the input directory.\n",
    "\n",
    "  Args:\n",
    "  - input_dir: Directory containing processed log CSV files.\n",
    "  - output_dir: Directory to save preprocessed CSV files.\n",
    "\n",
    "  Returns:\n",
    "  - None\n",
    "  \"\"\"\n",
    "  \n",
    "  if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "  # List all CSV files in the input directory\n",
    "  csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "\n",
    "  for file in csv_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Handle missing values\n",
    "    df['response_time'] = df['response_time'].fillna(0)  # Replace missing response times with 0\n",
    "\n",
    "    # Feature engineering\n",
    "    # Ensure timestamp is in datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "    # Sort by timestamp and calculate time delta\n",
    "    df = df.sort_values(by='timestamp')\n",
    "    df['time_delta'] = df['timestamp'].diff().dt.total_seconds()\n",
    "    df['time_delta'] = df['time_delta'].interpolate()\n",
    "    \n",
    "    # Check if internal_ip is present\n",
    "    if 'internal_ip' not in df.columns:\n",
    "      raise ValueError(\"internal_ip column is missing from the dataset!\")\n",
    "    \n",
    "    # Fill missing IPs with placeholder\n",
    "    df['internal_ip'] = df['internal_ip'].fillna('INTERNAL_PROCESS')\n",
    "\n",
    "    # Create an error flag\n",
    "    df['is_error'] = df['log_level'].isin(['ERROR', 'CRITICAL']).astype(int)\n",
    "\n",
    "    # Normalize response time\n",
    "    if 'response_time' in df.columns:\n",
    "      df['response_time_normalized'] = (\n",
    "        df['response_time'] - df['response_time'].mean()\n",
    "        ) / df['response_time'].std()\n",
    "\n",
    "    # Save preprocessed data\n",
    "    output_file = os.path.join(output_dir, f\"preprocessed_{os.path.splitext(file.rsplit('_', 1)[0])[0]}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Preprocessed file saved to: {output_file}\")\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_logs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking missing values in preprocessed/preprocessed_openstack_predict.csv\n",
      "request_id         1439\n",
      "user_id            1439\n",
      "project_id         1439\n",
      "request            9018\n",
      "status_code        9018\n",
      "response_length    9018\n",
      "time_delta            1\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Checking missing values in preprocessed/preprocessed_openstack_test.csv\n",
      "request_id          4215\n",
      "user_id             4215\n",
      "project_id          4215\n",
      "request            25678\n",
      "status_code        25688\n",
      "response_length    25688\n",
      "time_delta             1\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Checking missing values in preprocessed/preprocessed_openstack_train.csv\n",
      "request_id          9992\n",
      "user_id             9992\n",
      "project_id          9992\n",
      "request            74317\n",
      "status_code        74381\n",
      "response_length    74381\n",
      "time_delta             1\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# missing values check\n",
    "input_dir='preprocessed'\n",
    "\n",
    "processed_files = [\n",
    "    'preprocessed/preprocessed_openstack_predict.csv',\n",
    "    'preprocessed/preprocessed_openstack_test.csv',\n",
    "    'preprocessed/preprocessed_openstack_train.csv',\n",
    "]\n",
    "\n",
    "for file in processed_files:\n",
    "    \n",
    "    print(f\"Checking missing values in {file}\")\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    missing_summary = df.isnull().sum()\n",
    "    print(missing_summary[missing_summary > 0])  # Only display columns with missing values\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating data types in preprocessed/preprocessed_openstack_predict.csv\n",
      "Timestamp conversion successful: True\n",
      "response_time: Numeric check - True\n",
      "response_length: Numeric check - True\n",
      "\n",
      "\n",
      "Validating data types in preprocessed/preprocessed_openstack_test.csv\n",
      "Timestamp conversion successful: True\n",
      "response_time: Numeric check - True\n",
      "response_length: Numeric check - True\n",
      "\n",
      "\n",
      "Validating data types in preprocessed/preprocessed_openstack_train.csv\n",
      "Timestamp conversion successful: True\n",
      "response_time: Numeric check - True\n",
      "response_length: Numeric check - True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for valid data types\n",
    "\n",
    "for file in processed_files:\n",
    "  \n",
    "  print(f\"Validating data types in {file}\")\n",
    "  df = pd.read_csv(file)\n",
    "  \n",
    "  # Convert timestamp to datetime and check\n",
    "  df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "  print(\"Timestamp conversion successful:\", df['timestamp'].notnull().all())\n",
    "  \n",
    "  # Check numeric columns\n",
    "  numeric_columns = ['response_time', 'response_length']\n",
    "  \n",
    "  for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "      print(f\"{col}: Numeric check -\", pd.api.types.is_numeric_dtype(df[col]))\n",
    "  \n",
    "  print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating data ranges in preprocessed/preprocessed_openstack_predict.csv\n",
      "Negative response_time entries: 0\n",
      "Invalid status codes: [nan]\n",
      "Negative response_length entries: 0\n",
      "\n",
      "\n",
      "Validating data ranges in preprocessed/preprocessed_openstack_test.csv\n",
      "Negative response_time entries: 0\n",
      "Invalid status codes: [nan]\n",
      "Negative response_length entries: 0\n",
      "\n",
      "\n",
      "Validating data ranges in preprocessed/preprocessed_openstack_train.csv\n",
      "Negative response_time entries: 0\n",
      "Invalid status codes: [nan]\n",
      "Negative response_length entries: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validating expected ranges\n",
    "\n",
    "# predefine some known error codes\n",
    "defaultErrorCodes = [200, 201, 202, 204, 400, 401, 403, 404, 500, 503]\n",
    "\n",
    "for file in processed_files:\n",
    "  print(f\"Validating data ranges in {file}\")\n",
    "  df = pd.read_csv(file)\n",
    "  \n",
    "  # Check response_time\n",
    "  if 'response_time' in df.columns:\n",
    "      print(\"Negative response_time entries:\", (df['response_time'] < 0).sum())\n",
    "  \n",
    "  # Check status_code\n",
    "  if 'status_code' in df.columns:\n",
    "      print(\"Invalid status codes:\", df[~df['status_code'].isin(defaultErrorCodes)]['status_code'].unique())\n",
    "  \n",
    "  # Check response_length\n",
    "  if 'response_length' in df.columns:\n",
    "      print(\"Negative response_length entries:\", (df['response_length'] < 0).sum())\n",
    "  \n",
    "  print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: Index(['timestamp', 'log_level', 'source', 'request_id', 'user_id',\n",
      "       'project_id', 'internal_ip', 'request', 'status_code',\n",
      "       'response_length', 'response_time', 'time_delta', 'is_error',\n",
      "       'response_time_normalized'],\n",
      "      dtype='object')\n",
      "Test columns: Index(['timestamp', 'log_level', 'source', 'request_id', 'user_id',\n",
      "       'project_id', 'internal_ip', 'request', 'status_code',\n",
      "       'response_length', 'response_time', 'time_delta', 'is_error',\n",
      "       'response_time_normalized'],\n",
      "      dtype='object')\n",
      "Predict columns: Index(['timestamp', 'log_level', 'source', 'request_id', 'user_id',\n",
      "       'project_id', 'internal_ip', 'request', 'status_code',\n",
      "       'response_length', 'response_time', 'time_delta', 'is_error',\n",
      "       'response_time_normalized'],\n",
      "      dtype='object')\n",
      "All datasets have consistent columns!\n"
     ]
    }
   ],
   "source": [
    "# Compare columns across datasets\n",
    "\n",
    "df_train = pd.read_csv(\"preprocessed/preprocessed_openstack_predict.csv\")\n",
    "df_test = pd.read_csv(\"preprocessed/preprocessed_openstack_test.csv\")\n",
    "df_predict = pd.read_csv(\"preprocessed/preprocessed_openstack_train.csv\")\n",
    "\n",
    "print(\"Train columns:\", df_train.columns)\n",
    "print(\"Test columns:\", df_test.columns)\n",
    "print(\"Predict columns:\", df_predict.columns)\n",
    "\n",
    "# Check for differences in column sets\n",
    "assert set(df_train.columns) == set(df_test.columns), \"Train and Test columns mismatch!\"\n",
    "assert set(df_train.columns) == set(df_predict.columns), \"Train and Predict columns mismatch!\"\n",
    "print(\"All datasets have consistent columns!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Train dataset:\n",
      "       response_time  response_length\n",
      "count   18434.000000      9416.000000\n",
      "mean        0.576138      1449.752761\n",
      "std         2.998784      1378.649163\n",
      "min         0.000000       116.000000\n",
      "25%         0.000000       380.000000\n",
      "50%         0.088699      1892.000000\n",
      "75%         0.263785      1893.000000\n",
      "max        51.590000     24904.000000\n",
      "\n",
      "\n",
      "Statistics for Test dataset:\n",
      "       response_time  response_length\n",
      "count   52312.000000     26624.000000\n",
      "mean        0.563563      1436.455341\n",
      "std         2.898609      1398.752157\n",
      "min         0.000000       116.000000\n",
      "25%         0.000000       380.000000\n",
      "50%         0.085623      1892.000000\n",
      "75%         0.263155      1893.000000\n",
      "max        22.910000     24904.000000\n",
      "\n",
      "\n",
      "Statistics for Predict dataset:\n",
      "       response_time  response_length\n",
      "count  137074.000000     62693.000000\n",
      "mean        0.508737      1439.021039\n",
      "std         2.758057      1399.633460\n",
      "min         0.000000       116.000000\n",
      "25%         0.000000       380.000000\n",
      "50%         0.000000      1892.000000\n",
      "75%         0.261537      1893.000000\n",
      "max        23.340000     24904.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Numeric features to compare\n",
    "numeric_features = ['response_time', 'response_length']\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    \"Train\": df_train,\n",
    "    \"Test\": df_test,\n",
    "    \"Predict\": df_predict\n",
    "}\n",
    "\n",
    "# Calculate summary statistics\n",
    "for name, df in datasets.items():\n",
    "    print(f\"Statistics for {name} dataset:\")\n",
    "    print(df[numeric_features].describe())\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
